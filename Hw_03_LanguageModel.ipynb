{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVGBTApqf0d0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEZ0ffOff0d2"
      },
      "source": [
        "### Загрузить датасет [1 балл]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PYNlYm0f0d5",
        "outputId": "36877ef6-7bcb-4e74-e07f-54b4ea6a752f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "15259                         [{'name': 'Eric Laporte'}]   21   0711.3449v1   \n",
              "31586  [{'name': 'Mohammed E. Fathy'}, {'name': 'Quoc...   20  1803.07231v1   \n",
              "12788  [{'name': 'Andre Viebke'}, {'name': 'Suejb Mem...   25  1702.07908v1   \n",
              "28821  [{'name': 'Bingke Zhu'}, {'name': 'Yingying Ch...   26  1707.08289v1   \n",
              "21521  [{'name': 'Nicole Kraemer'}, {'name': 'Masashi...   19   0902.3347v1   \n",
              "\n",
              "                                                    link  month  \\\n",
              "15259  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "31586  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "12788  [{'rel': 'related', 'href': 'http://dx.doi.org...      2   \n",
              "28821  [{'rel': 'alternate', 'href': 'http://arxiv.or...      7   \n",
              "21521  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "\n",
              "                                                 summary  \\\n",
              "15259  International standards for lexicon formats ar...   \n",
              "31586  Interest point descriptors have fueled progres...   \n",
              "12788  Deep learning is an important component of big...   \n",
              "28821  Image matting plays an important role in image...   \n",
              "21521  The runtime for Kernel Partial Least Squares (...   \n",
              "\n",
              "                                                     tag  \\\n",
              "15259  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "31586  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "12788  [{'term': 'cs.DC', 'scheme': 'http://arxiv.org...   \n",
              "28821  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "21521  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "\n",
              "                                                   title  year  \n",
              "15259            Lexicon management and standard formats  2007  \n",
              "31586  Hierarchical Metric Learning and Matching for ...  2018  \n",
              "12788  CHAOS: A Parallelization Scheme for Training C...  2017  \n",
              "28821  Fast Deep Matting for Portrait Animation on Mo...  2017  \n",
              "21521  Lanczos Approximations for the Speedup of Kern...  2009  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da3a0b7d-f9a2-488b-91df-0ce7e76e5129\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15259</th>\n",
              "      <td>[{'name': 'Eric Laporte'}]</td>\n",
              "      <td>21</td>\n",
              "      <td>0711.3449v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>International standards for lexicon formats ar...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Lexicon management and standard formats</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31586</th>\n",
              "      <td>[{'name': 'Mohammed E. Fathy'}, {'name': 'Quoc...</td>\n",
              "      <td>20</td>\n",
              "      <td>1803.07231v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>Interest point descriptors have fueled progres...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Hierarchical Metric Learning and Matching for ...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12788</th>\n",
              "      <td>[{'name': 'Andre Viebke'}, {'name': 'Suejb Mem...</td>\n",
              "      <td>25</td>\n",
              "      <td>1702.07908v1</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>2</td>\n",
              "      <td>Deep learning is an important component of big...</td>\n",
              "      <td>[{'term': 'cs.DC', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>CHAOS: A Parallelization Scheme for Training C...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28821</th>\n",
              "      <td>[{'name': 'Bingke Zhu'}, {'name': 'Yingying Ch...</td>\n",
              "      <td>26</td>\n",
              "      <td>1707.08289v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>7</td>\n",
              "      <td>Image matting plays an important role in image...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Fast Deep Matting for Portrait Animation on Mo...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21521</th>\n",
              "      <td>[{'name': 'Nicole Kraemer'}, {'name': 'Masashi...</td>\n",
              "      <td>19</td>\n",
              "      <td>0902.3347v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>The runtime for Kernel Partial Least Squares (...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Lanczos Approximations for the Speedup of Kern...</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da3a0b7d-f9a2-488b-91df-0ce7e76e5129')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da3a0b7d-f9a2-488b-91df-0ce7e76e5129 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da3a0b7d-f9a2-488b-91df-0ce7e76e5129');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-966fe0d8-5fe2-4e29-9050-a4e1a27ce2c1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-966fe0d8-5fe2-4e29-9050-a4e1a27ce2c1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-966fe0d8-5fe2-4e29-9050-a4e1a27ce2c1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'name': 'Mohammed E. Fathy'}, {'name': 'Quoc-Huy Tran'}, {'name': 'M. Zeeshan Zia'}, {'name': 'Paul Vernaza'}, {'name': 'Manmohan Chandraker'}]\",\n          \"[{'name': 'Nicole Kraemer'}, {'name': 'Masashi Sugiyama'}, {'name': 'Mikio Braun'}]\",\n          \"[{'name': 'Andre Viebke'}, {'name': 'Suejb Memeti'}, {'name': 'Sabri Pllana'}, {'name': 'Ajith Abraham'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 19,\n        \"max\": 26,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          20,\n          19,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1803.07231v1\",\n          \"0902.3347v1\",\n          \"1702.07908v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1803.07231v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1803.07231v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/0902.3347v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/0902.3347v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'related', 'href': 'http://dx.doi.org/10.1007/s11227-017-1994-x', 'type': 'text/html', 'title': 'doi'}, {'rel': 'alternate', 'href': 'http://arxiv.org/abs/1702.07908v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1702.07908v1', 'type': 'application/pdf', 'title': 'pdf'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 11,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          7,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Interest point descriptors have fueled progress on almost every problem in\\ncomputer vision. Recent advances in deep neural networks have enabled\\ntask-specific learned descriptors that outperform hand-crafted descriptors on\\nmany problems. We demonstrate that commonly used metric learning approaches do\\nnot optimally leverage the feature hierarchies learned in a Convolutional\\nNeural Network (CNN), especially when applied to the task of geometric feature\\nmatching. While a metric loss applied to the deepest layer of a CNN, is often\\nexpected to yield ideal features irrespective of the task, in fact the growing\\nreceptive field as well as striding effects cause shallower features to be\\nbetter at high precision matching tasks. We leverage this insight together with\\nexplicit supervision at multiple levels of the feature hierarchy for better\\nregularization, to learn more effective descriptors in the context of geometric\\nmatching tasks. Further, we propose to use activation maps at different layers\\nof a CNN, as an effective and principled replacement for the multi-resolution\\nimage pyramids often used for matching tasks. We propose concrete CNN\\narchitectures employing these ideas, and evaluate them on multiple datasets for\\n2D and 3D geometric matching as well as optical flow, demonstrating\\nstate-of-the-art results and generalization across datasets.\",\n          \"The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is\\nquadratic in the number of examples. However, the necessity of obtaining\\nsensitivity measures as degrees of freedom for model selection or confidence\\nintervals for more detailed analysis requires cubic runtime, and thus\\nconstitutes a computational bottleneck in real-world data analysis. We propose\\na novel algorithm for KPLS which not only computes (a) the fit, but also (b)\\nits approximate degrees of freedom and (c) error bars in quadratic runtime. The\\nalgorithm exploits a close connection between Kernel PLS and the Lanczos\\nalgorithm for approximating the eigenvalues of symmetric matrices, and uses\\nthis approximation to compute the trace of powers of the kernel matrix in\\nquadratic runtime.\",\n          \"Deep learning is an important component of big-data analytic tools and\\nintelligent applications, such as, self-driving cars, computer vision, speech\\nrecognition, or precision medicine. However, the training process is\\ncomputationally intensive, and often requires a large amount of time if\\nperformed sequentially. Modern parallel computing systems provide the\\ncapability to reduce the required training time of deep neural networks. In\\nthis paper, we present our parallelization scheme for training convolutional\\nneural networks (CNN) named Controlled Hogwild with Arbitrary Order of\\nSynchronization (CHAOS). Major features of CHAOS include the support for thread\\nand vector parallelism, non-instant updates of weight parameters during\\nback-propagation without a significant delay, and implicit synchronization in\\narbitrary order. CHAOS is tailored for parallel computing systems that are\\naccelerated with the Intel Xeon Phi. We evaluate our parallelization approach\\nempirically using measurement techniques and performance modeling for various\\nnumbers of threads and CNN architectures. Experimental results for the MNIST\\ndataset of handwritten digits using the total number of threads on the Xeon Phi\\nshow speedups of up to 103x compared to the execution on one thread of the Xeon\\nPhi, 14x compared to the sequential execution on Intel Xeon E5, and 58x\\ncompared to the sequential execution on Intel Core i5.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Hierarchical Metric Learning and Matching for 2D and 3D Geometric\\n  Correspondences\",\n          \"Lanczos Approximations for the Speedup of Kernel Partial Least Squares\\n  Regression\",\n          \"CHAOS: A Parallelization Scheme for Training Convolutional Neural\\n  Networks on Intel Xeon Phi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 2007,\n        \"max\": 2018,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2018,\n          2009,\n          2007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "# !wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "# !tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfPLcf-nf0d8"
      },
      "source": [
        "Немножко запрепроцессим данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu2o1JkXf0d9",
        "outputId": "07ae0df5-fa20-46e3-dcb9-8f9711495648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "old_lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
        "\n",
        "sorted(old_lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7wM8CNkf0eC"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tk = WordPunctTokenizer()\n",
        "lines = [] # Tokenize, replace \\n with space, lower sentence and join using space\n",
        "for line in old_lines:\n",
        "    new_line = ' '.join([cur_line.lower() for cur_line in tk.tokenize(line)])\n",
        "    lines.append(new_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoTjpDIbf0eC",
        "outputId": "df9fc843-43b6-490b-b7ca-bc5c2b95a962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'differential contrastive divergence ; this paper has been retracted .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sorted(lines, key=len)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzCL1rvff0eC"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == \\\n",
        "    'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == \\\n",
        "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htqk44H4f0eD"
      },
      "source": [
        " ### Посчитаем все возможные n граммы [2 балла]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUzbMXL3f0eD"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# special tokens:\n",
        "# - unk represents absent tokens,\n",
        "# - eos is a special token after the end of sequence\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines: list[str], n: int):\n",
        "    \"\"\"\n",
        "    Count how many times each word occured after (n - 1) previous words\n",
        "    :param lines: an iterable of strings with space-separated tokens\n",
        "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
        "\n",
        "    When building counts, please consider the following two edge cases\n",
        "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
        "      empty prefix: \"\" -> (UNK, UNK)\n",
        "      short prefix: \"the\" -> (UNK, the)\n",
        "      long prefix: \"the new approach\" -> (new, approach)\n",
        "    - you should add a special token, EOS, at the end of each sequence\n",
        "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
        "      count the probability of this token just like all others.\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        for i, last_token in enumerate(tokens):\n",
        "            tokens_tuple = tuple([UNK] * max(0, (n - 1 - (i + 1) + 1)) + tokens[max(0, i - n + 1):max(0, i) ])\n",
        "            counts[tokens_tuple][last_token] += 1\n",
        "        i = len(tokens)\n",
        "        tokens_tuple = tuple([UNK] * max(0, (n - 1 - (i + 1) + 1)) + tokens[max(0, i - n + 1):max(0, i) ])\n",
        "        counts[tokens_tuple][EOS] += 1\n",
        "\n",
        "\n",
        "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
        "\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_kBo9o8f0eF"
      },
      "outputs": [],
      "source": [
        "# let's test it\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiMK9qScNy4k",
        "outputId": "29e72997-d42a-43f6-8d21-c3c960e55216"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dummy_counts['p', '='].total()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_FpBKqaf0eG"
      },
      "source": [
        "### Реализовать get_possible_next_tokens и инициализацию [2 балл]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3LG91lmNy4k"
      },
      "outputs": [],
      "source": [
        "probs = {}\n",
        "for prefix, contexts in dummy_counts.items():\n",
        "    probs  = {prefix:{elem:contexts[elem] / contexts.total() for elem in contexts.keys()}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oqwJn9vf0eG"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:\n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\"\n",
        "        Train a simple count-based language model:\n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "\n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "\n",
        "        # compute token proabilities given counts\n",
        "        self.probs = {}\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "\n",
        "        # populate self.probs with actual probabilities\n",
        "        for prefix, contexts in counts.items():\n",
        "            self.probs[prefix] = {elem:contexts[elem] / contexts.total() for elem in contexts.keys()}\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix) -> dict[str, float]:\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        tokens = prefix.split()\n",
        "        i = len(tokens)\n",
        "        tokens_tuple = tuple([UNK] * max(0, (self.n - 1 - (i + 1) + 1)) + tokens[max(0, i - self.n + 1):max(0, i) ])\n",
        "\n",
        "\n",
        "        return self.probs[tokens_tuple]\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token) -> float:\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ornS7Xs8f0eG"
      },
      "outputs": [],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwuaJGmYf0eG"
      },
      "outputs": [],
      "source": [
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXrSXIO9f0eH"
      },
      "source": [
        "### Реализуйте get_next_token с сэмплингом по вероятностям и температурой. [2 балла]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTbng2idf0eH"
      },
      "outputs": [],
      "source": [
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    probs_tokens = lm.get_possible_next_tokens(prefix)\n",
        "    tokens = list(probs_tokens.keys())\n",
        "    if temperature == 0:\n",
        "        max_prob = 0\n",
        "        best_token = None\n",
        "        for token in tokens:\n",
        "            if probs_tokens[token] > max_prob:\n",
        "                best_token = token\n",
        "                max_prob = probs_tokens[token]\n",
        "        return best_token\n",
        "    new_probs = np.array([probs_tokens[token] ** (1 / temperature ) for token in tokens ])\n",
        "    new_probs = new_probs/new_probs.sum()\n",
        "    return np.random.choice(tokens, p = new_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAkH3ZQnf0eH",
        "outputId": "1d7f1db2-69d1-47ad-f335-0407fe5d0bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks nice!\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8400 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5s5gWK1f0eH",
        "outputId": "4b5e5752-5385-426e-988e-5fa65354c07f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial neural network ( dnn )- based dnn baseline . _EOS_\n"
          ]
        }
      ],
      "source": [
        "prefix = 'artificial' # <- your ideas :)\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWeQSXnVf0eI",
        "outputId": "cb25b379-bf4a-4157-b57d-8350ca00c1f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bridging the gap between the points ' method ( t - sne to randomly generate environments that are not only allows for a variety of applications , e . g ., the syntactic and semantic segmentation and classification ; this paper , we propose a new framework for the case of the art performance on a set of problems in artificial intelligence , which is a crucial role in the literature . we propose a new approach to approximate the optimal solution . we also present a new approach for learning . the proposed algorithm achieves a competitive performance . _EOS_\n"
          ]
        }
      ],
      "source": [
        "prefix = 'bridging the' # <- more of your ideas\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XpKPRV4dZuJ"
      },
      "source": [
        "### Также в нашей задаче может пригодиться perplexity. Добавьте её вычисление в класс `NGramLanguageModel` [1 балл]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReS2truEf0eI"
      },
      "source": [
        "### Реализуйте инициализацию и get_possible_next_tokens так, чтобы получилась нграмная модель с Лапласовским сглаживанием [2 балла]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgEnaSDff0eI"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel):\n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta = 0.1):\n",
        "        super().__init__(lines, n)\n",
        "\n",
        "        self.probs = {}\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "\n",
        "        all_tokens = set()\n",
        "        for prefix, contexts in counts.items():\n",
        "            for key in contexts.keys():\n",
        "                all_tokens.add(key)\n",
        "\n",
        "        for prefix, contexts in counts.items():\n",
        "            self.probs[prefix] = {elem:(contexts[elem] + delta)/ (contexts.total() + delta * len(all_tokens)) for elem in contexts.keys()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG3kRtn4f0eL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz8w0fLef0eM"
      },
      "source": [
        "### Будем работать с Char-Level моделями, поэтому можем позволить себе все буквы английского алфавита (даже двух регистров). [1 балл]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zWH-Zbmf0eM"
      },
      "outputs": [],
      "source": [
        "BOS, EOS = ' ', '\\n'\n",
        "\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:512], axis=1) \\\n",
        "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
        "            .tolist()\n",
        "\n",
        "# if you missed the seminar, download data here - https://yadi.sk/d/_nGyU2IajjR9-w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7KB9m7Sf0eM",
        "outputId": "2d17f707-38bc-44fc-9441-a72cbf62ec8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens =  136\n"
          ]
        }
      ],
      "source": [
        "# get all unique characters from lines (including capital letters and symbols)\n",
        "tokens = list({char for line in lines for char in line})\n",
        "\n",
        "tokens = sorted(tokens)\n",
        "n_tokens = len(tokens)\n",
        "print ('n_tokens = ',n_tokens)\n",
        "assert 100 < n_tokens < 150\n",
        "assert BOS in tokens, EOS in tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzXNQrlRf0eM"
      },
      "outputs": [],
      "source": [
        "# dictionary of character -> its identifier (index in tokens list)\n",
        "token_to_id = {char: idx for idx, char in enumerate(tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeTm2uO9f0eN",
        "outputId": "27580029-0bef-4482-dd0a-6ff5b9618cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems alright!\n"
          ]
        }
      ],
      "source": [
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
        "for i in range(n_tokens):\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
        "\n",
        "print(\"Seems alright!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vna2QTRJf0eN"
      },
      "outputs": [],
      "source": [
        "def to_matrix(lines, max_len=None, pad=token_to_id[EOS], dtype=np.int64):\n",
        "    \"\"\"Casts a list of lines into torch-digestable matrix\"\"\"\n",
        "    max_len = max_len or max(map(len, lines))\n",
        "    lines_ix = np.full([len(lines), max_len], pad, dtype=dtype)\n",
        "    for i in range(len(lines)):\n",
        "        line_ix = list(map(token_to_id.get, lines[i][:max_len]))\n",
        "        lines_ix[i, :len(line_ix)] = line_ix\n",
        "    return lines_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqqfqgAwf0eN",
        "outputId": "9a797203-8cd1-4606-9c7c-8042b9031f46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n"
          ]
        }
      ],
      "source": [
        "#Example: cast 4 random names to matrices, pad with zeros\n",
        "dummy_lines = [\n",
        "    ' abc\\n',\n",
        "    ' abacaba\\n',\n",
        "    ' abc1234567890\\n',\n",
        "]\n",
        "print(to_matrix(dummy_lines))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnWuQUJ5f0eN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywRT8wgDf0eO",
        "outputId": "477fc770-da6a-46cc-c212-ab371b16b549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix:\n",
            " [[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n",
            " [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n",
            "mask: [[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "lengths: [ 5  9 15]\n"
          ]
        }
      ],
      "source": [
        "dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n",
        "def compute_mask(input_ix, eos_ix=token_to_id[EOS]):\n",
        "    \"\"\" compute a boolean mask that equals \"1\" until first EOS (including that EOS) \"\"\"\n",
        "    return F.pad(torch.cumsum(input_ix == eos_ix, dim=-1)[..., :-1] < 1, pad=(1, 0, 0, 0), value=True)\n",
        "\n",
        "print('matrix:\\n', dummy_input_ix.numpy())\n",
        "print('mask:', compute_mask(dummy_input_ix).to(torch.int32).cpu().numpy())\n",
        "print('lengths:', compute_mask(dummy_input_ix).sum(-1).cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4GkrAQFf0eO"
      },
      "source": [
        "### Реализуйте CrossEntropyLoss [1 балл]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuEy87yllV76"
      },
      "source": [
        "$$L(\\hat{y},y) = -\\sum\\limits_k^K {y^{(k)} \\log{\\hat{y}} ^ {(k)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZARouhsdf0eO"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, input_ix):\n",
        "    \"\"\"\n",
        "    :param model: language model that can compute next token logits given token indices\n",
        "    :param input ix: int32 matrix of tokens, shape: [batch_size, length]; padded with eos_ix\n",
        "    :returns: scalar loss function, mean crossentropy over non-eos tokens\n",
        "    \"\"\"\n",
        "\n",
        "    input_ix = torch.as_tensor(input_ix, dtype=torch.long)\n",
        "\n",
        "    logits = model(input_ix[:, :-1])\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "    label = input_ix[:, 1:]\n",
        "\n",
        "    mask = compute_mask(label)\n",
        "\n",
        "    extracted_loss = -torch.log(probs.gather(-1, label.unsqueeze(-1)).squeeze(-1)) * mask\n",
        "\n",
        "    # TODO\n",
        "    # Your task: implement loss function as per formula above\n",
        "    # your loss should only be computed on actual tokens, excluding padding\n",
        "    # predicting actual tokens and first EOS do count. Subsequent EOS-es don't\n",
        "    # you may or may not want to use the compute_mask function from above.\n",
        "    return extracted_loss.sum() / mask.sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brsJKCnIf0eP"
      },
      "source": [
        "### Реализуйте инициализацию и метод forward. Можно разобраться с pack_padded_sequence и pad_packed_sequence [3 балла]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDZ-BDwqf0eP"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, n_tokens=n_tokens, emb_size=16, hid_size=256):\n",
        "        \"\"\"\n",
        "        Build a recurrent language model.\n",
        "        You are free to choose anything you want, but the recommended architecture is\n",
        "        - token embeddings\n",
        "        - one or more LSTM/GRU layers with hid size\n",
        "        - linear layer to predict logits\n",
        "\n",
        "        :note: if you use nn.RNN/GRU/LSTM, make sure you specify batch_first=True\n",
        "         With batch_first, your model operates with tensors of shape [batch_size, sequence_length, num_units]\n",
        "         Also, please read the docs carefully: they don't just return what you want them to return :)\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
        "\n",
        "\n",
        "        self.emb = nn.Embedding(n_tokens, embedding_dim=emb_size, padding_idx=token_to_id[BOS])\n",
        "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hid_size, num_layers=1, batch_first=True)\n",
        "        self.lin = nn.Linear(hid_size, n_tokens)\n",
        "\n",
        "    def forward(self, input_ix):\n",
        "        \"\"\"\n",
        "        compute language model logits given input tokens\n",
        "        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n",
        "        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n",
        "            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n",
        "        \"\"\"\n",
        "\n",
        "        embs = self.emb(input_ix)\n",
        "        out, _ = self.lstm(embs)\n",
        "        logits = self.lin(out)\n",
        "        return logits\n",
        "\n",
        "        # output tensor should be of shape [batch_size, sequence_length, n_tokens]\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix=BOS, max_len=100) -> dict[str, float]:\n",
        "        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n",
        "        prefix_idxs = torch.tensor([token_to_id[token] for token in prefix[max(0, len(prefix) - max_len):]]).to('cuda')\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(prefix_idxs)\n",
        "            token_vs_prob = nn.functional.softmax(probs)[-1]\n",
        "        return {prefix[i] : token_vs_prob[i] for i in range(len(probs))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkKIyRqLf0eP",
        "outputId": "ae915202-0ff8-4dbd-eed2-fac79550f7e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ('emb.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lin.weight', 'lin.bias')\n"
          ]
        }
      ],
      "source": [
        "model = RNNLanguageModel()\n",
        "\n",
        "dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n",
        "dummy_logits = model(dummy_input_ix)\n",
        "\n",
        "assert isinstance(dummy_logits, torch.Tensor)\n",
        "assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n",
        "assert not np.allclose(dummy_logits.cpu().data.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\"\n",
        "print('Weights:', tuple(name for name, w in model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxw0Tqhyf0eP"
      },
      "outputs": [],
      "source": [
        "# test for lookahead\n",
        "dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n",
        "dummy_logits_2 = model(dummy_input_ix_2)\n",
        "\n",
        "assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n",
        "    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n",
        "    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJAE-LTWf0eP"
      },
      "source": [
        "### Реализовать части generate [2 балла]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZAHvuA8f0eQ"
      },
      "outputs": [],
      "source": [
        "def score_lines(model, dev_lines, batch_size):\n",
        "    \"\"\" computes average loss over the entire dataset \"\"\"\n",
        "    dev_loss_num, dev_loss_len = 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(dev_lines), batch_size):\n",
        "            batch_ix = to_matrix(dev_lines[i: i + batch_size])\n",
        "            dev_loss_num += compute_loss(model, batch_ix).item() * len(batch_ix)\n",
        "            dev_loss_len += len(batch_ix)\n",
        "    return dev_loss_num / dev_loss_len\n",
        "\n",
        "def generate(model, prefix=BOS, temperature=1.0, max_len=100):\n",
        "    \"\"\"\n",
        "    Samples output sequence from probability distribution obtained by model\n",
        "    :param temperature: samples proportionally to model probabilities ^ temperature\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            #TODO\n",
        "            tokens_probs = model.get_possible_next_tokens(prefix)\n",
        "            tokens = list(tokens_probs.keys())\n",
        "            probs = np.array([tokens_probs[token] for token in tokens])\n",
        "            if temperature == 0:\n",
        "                next_token = tokens[torch.argmax(probs)]\n",
        "            else:\n",
        "                new_probs = np.array([prob ** (1 / temperature) for prob in probs ])\n",
        "                new_probs = new_probs/new_probs.sum()\n",
        "                next_token =  np.random.choice(tokens, p = new_probs)\n",
        "            prefix += next_token\n",
        "            if next_token == EOS or len(prefix) > max_len:\n",
        "                break\n",
        "    return prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHhugGbLf0eQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, dev_lines = train_test_split(lines, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62w5x5f7f0eQ"
      },
      "outputs": [],
      "source": [
        "batch_size = 64      # <-- please tune batch size to fit your CPU/GPU configuration\n",
        "score_dev_every = 250\n",
        "train_history, dev_history = [], []\n",
        "\n",
        "model = RNNLanguageModel()\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# score untrained model\n",
        "dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n",
        "# print(\"Sample before training:\", generate(model, 'Bridging'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJtGVU8Nf0eQ"
      },
      "source": [
        "### Чуть-чуть напишите тренировку [1 балл]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WDhEBK_f0eQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "from tqdm import trange\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "batches = []\n",
        "for i in range(0, len(train_lines), batch_size):\n",
        "    batches.append(torch.tensor(to_matrix(train_lines[i:i + batch_size])))\n",
        "\n",
        "for i in trange(len(train_history), 5000):\n",
        "    epoch_losses = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = compute_loss(model, batch)\n",
        "        epoch_losses.append(loss)\n",
        "        opt.step()\n",
        "\n",
        "    train_history.append((i, float(np.mean(epoch_losses))))\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        clear_output(True)\n",
        "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
        "        if len(dev_history):\n",
        "            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
        "        plt.legend(); plt.grid(); plt.show()\n",
        "        print(\"Generated examples (tau=0.5):\")\n",
        "        for _ in range(3):\n",
        "            print(generate(model, temperature=0.5))\n",
        "\n",
        "    if (i + 1) % score_dev_every == 0:\n",
        "        print(\"Scoring dev...\")\n",
        "        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n",
        "        print('#%i Dev loss: %.3f' % dev_history[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrtJDof5f0eR"
      },
      "outputs": [],
      "source": [
        "assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n",
        "print(\"Final dev loss:\", dev_history[-1][-1])\n",
        "for i in range(10):\n",
        "    print(generate(model, temperature=0.5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}